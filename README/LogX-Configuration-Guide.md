# LogX é…ç½®è¯¦è§£ä¸éƒ¨ç½²å®Œæ•´æŒ‡å—

## ğŸ“‘ ç›®å½•

- [é…ç½®æ–‡ä»¶å®Œæ•´ç¤ºä¾‹](#é…ç½®æ–‡ä»¶å®Œæ•´ç¤ºä¾‹)
- [ä¸­é—´ä»¶é…ç½®è¯¦è§£](#ä¸­é—´ä»¶é…ç½®è¯¦è§£)
- [Elasticsearch ç´¢å¼•è®¾è®¡](#elasticsearch-ç´¢å¼•è®¾è®¡)
- [gRPC åè®®è¯´æ˜](#grpc-åè®®è¯´æ˜)
- [ä¸€é”®éƒ¨ç½²è„šæœ¬](#ä¸€é”®éƒ¨ç½²è„šæœ¬)
- [ç›‘æ§ä¸è¿ç»´](#ç›‘æ§ä¸è¿ç»´)

---

## é…ç½®æ–‡ä»¶å®Œæ•´ç¤ºä¾‹

### 1. å•ä½“åº”ç”¨é…ç½® (logx-standalone/application.yml)

```yaml
server:
  port: 8080

spring:
  application:
    name: logx-standalone

  # ==================== æ•°æ®æºé…ç½® ====================
  datasource:
    driver-class-name: com.mysql.cj.jdbc.Driver
    url: jdbc:mysql://localhost:3307/logx?useUnicode=true&characterEncoding=utf8&serverTimezone=Asia/Shanghai&useSSL=false
    username: root
    password: root123
    druid:
      initial-size: 5
      min-idle: 5
      max-active: 20
      max-wait: 60000
      time-between-eviction-runs-millis: 60000
      min-evictable-idle-time-millis: 300000
      validation-query: SELECT 1
      test-while-idle: true
      test-on-borrow: false
      test-on-return: false

  # ==================== Redis é…ç½® ====================
  data:
    redis:
      host: localhost
      port: 6379
      password: redis123
      database: 0
      lettuce:
        pool:
          max-active: 8
          max-idle: 8
          min-idle: 2
          max-wait: -1ms
      timeout: 5000ms

    # ==================== Elasticsearch é…ç½® ====================
    elasticsearch:
      uris: http://localhost:9200
      username: elastic
      password: 8rc3Jl1jlAK3uVZZyhF4
      connection-timeout: 10000
      socket-timeout: 30000

  # ==================== Kafka é…ç½® ====================
  kafka:
    bootstrap-servers: localhost:29092
    
    # ç”Ÿäº§è€…é…ç½®
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.apache.kafka.common.serialization.StringSerializer
      acks: 1                        # 1=leaderç¡®è®¤, all=æ‰€æœ‰å‰¯æœ¬ç¡®è®¤
      retries: 3                     # é‡è¯•æ¬¡æ•°
      batch-size: 16384              # æ‰¹é‡å¤§å° (16KB)
      buffer-memory: 33554432        # ç¼“å†²å†…å­˜ (32MB)
      compression-type: lz4          # å‹ç¼©ç®—æ³•: none, gzip, snappy, lz4, zstd
      linger-ms: 10                  # æ‰¹é‡å‘é€å»¶è¿Ÿ
    
    # æ¶ˆè´¹è€…é…ç½®
    consumer:
      group-id: logx-consumer-group
      auto-offset-reset: latest      # earliest=ä»å¤´å¼€å§‹, latest=ä»æœ€æ–°
      enable-auto-commit: false      # æ‰‹åŠ¨æäº¤offset
      max-poll-records: 500          # å•æ¬¡æœ€å¤šæ‹‰å–500æ¡
      fetch-min-size: 1024           # æœ€å°æ‹‰å–1KB
      fetch-max-wait: 500            # æœ€å¤§ç­‰å¾…500ms
      concurrency: 3                 # å¹¶å‘æ¶ˆè´¹è€…æ•°é‡

# ==================== MyBatis Plus é…ç½® ====================
mybatis-plus:
  mapper-locations: classpath*:/mapper/**/*.xml
  type-aliases-package: com.domidodo.logx.*.entity
  configuration:
    map-underscore-to-camel-case: true
    log-impl: org.apache.ibatis.logging.slf4j.Slf4jImpl
  global-config:
    db-config:
      id-type: auto
      logic-delete-field: deleted
      logic-delete-value: 1
      logic-not-delete-value: 0

# ==================== MinIO é…ç½® ====================
minio:
  endpoint: http://localhost:9000
  access-key: admin
  secret-key: admin123
  bucket-name: logx-archive
  region: us-east-1

# ==================== LogX ä¸šåŠ¡é…ç½® ====================
logx:
  # ---------- å­˜å‚¨é…ç½® ----------
  storage:
    # ç´¢å¼•é…ç½®
    index:
      prefix: logx-logs              # ç´¢å¼•å‰ç¼€
      shards: 5                      # ä¸»åˆ†ç‰‡æ•°
      replicas: 1                    # å‰¯æœ¬æ•°
      refresh-interval: 5s           # åˆ·æ–°é—´éš”
    
    # ç”Ÿå‘½å‘¨æœŸé…ç½®
    lifecycle:
      hot-data-days: 7               # çƒ­æ•°æ®ä¿ç•™å¤©æ•° (é«˜æ€§èƒ½SSD)
      warm-data-days: 30             # æ¸©æ•°æ®ä¿ç•™å¤©æ•° (æ™®é€šç£ç›˜)
      cold-data-days: 90             # å†·æ•°æ®ä¿ç•™å¤©æ•° (å½’æ¡£å­˜å‚¨)
      cleanup-enabled: true          # æ˜¯å¦å¯ç”¨è‡ªåŠ¨æ¸…ç†
      cleanup-cron: "0 0 2 * * ?"    # æ¸…ç†ä»»åŠ¡cronè¡¨è¾¾å¼ (æ¯å¤©å‡Œæ™¨2ç‚¹)
      archive-enabled: true          # æ˜¯å¦å¯ç”¨å½’æ¡£
      archive-cron: "0 0 3 * * ?"    # å½’æ¡£ä»»åŠ¡cronè¡¨è¾¾å¼ (æ¯å¤©å‡Œæ™¨3ç‚¹)
    
    # å‹ç¼©é…ç½®
    compression:
      enabled: true                  # æ˜¯å¦å¯ç”¨å‹ç¼©
      algorithm: gzip                # å‹ç¼©ç®—æ³•: gzip, lz4
      level: 6                       # å‹ç¼©çº§åˆ« (1-9, æ•°å­—è¶Šå¤§å‹ç¼©ç‡è¶Šé«˜ä½†é€Ÿåº¦è¶Šæ…¢)
    
    # æ‰¹é‡æ“ä½œé…ç½®
    bulk:
      size: 1000                     # æ‰¹é‡å¤§å°
      flush-interval: 5m             # åˆ·æ–°é—´éš”
      concurrent-requests: 2         # å¹¶å‘è¯·æ±‚æ•°

  # ---------- é™æµé…ç½® ----------
  rate-limit:
    enabled: true
    global-qps: 10000                # å…¨å±€æ¯ç§’è¯·æ±‚æ•°
    tenant-qps: 1000                 # ç§Ÿæˆ·æ¯ç§’è¯·æ±‚æ•°
    system-qpm: 5000                 # ç³»ç»Ÿæ¯åˆ†é’Ÿè¯·æ±‚æ•°

  # ---------- Kafka Topic é…ç½® ----------
  kafka:
    topics:
      logs: logx-logs                # æ—¥å¿—ä¸»é¢˜
      alerts: logx-alerts            # å‘Šè­¦ä¸»é¢˜
    partitions: 3                    # åˆ†åŒºæ•°
    replication-factor: 1            # å‰¯æœ¬å› å­

# ==================== æ—¥å¿—é…ç½® ====================
logging:
  level:
    root: INFO
    com.domidodo.logx: DEBUG
    org.springframework.kafka: WARN
    org.elasticsearch: WARN
  pattern:
    console: "%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{50} - %msg%n"
  file:
    name: logs/logx-standalone.log
    max-size: 100MB
    max-history: 30

# ==================== æ¥å£æ–‡æ¡£é…ç½® ====================
knife4j:
  enable: true
  setting:
    language: zh_CN
    swagger-model-name: å®ä½“ç±»åˆ—è¡¨
```

---

### 2. ç½‘å…³æœåŠ¡é…ç½® (logx-gateway-http/application.yml)

```yaml
server:
  port: 10240

spring:
  application:
    name: logx-gateway-http

  # æ•°æ®æº (ä»…ç”¨äºéªŒè¯API Key)
  datasource:
    driver-class-name: com.mysql.cj.jdbc.Driver
    url: jdbc:mysql://localhost:3307/logx?useUnicode=true&characterEncoding=utf8
    username: root
    password: root123

  # Redis (ç”¨äºé™æµ)
  data:
    redis:
      host: localhost
      port: 6379
      password: redis123

  # Kafka (å‘é€æ—¥å¿—)
  kafka:
    bootstrap-servers: localhost:29092
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.apache.kafka.common.serialization.StringSerializer
      acks: 1
      retries: 3

# é™æµé…ç½®
logx:
  rate-limit:
    enabled: true
    global-qps: 10000
    tenant-qps: 1000

logging:
  level:
    com.domidodo.logx: DEBUG
  file:
    name: logs/gateway-http.log
```

---

### 3. æ—¥å¿—å¤„ç†å™¨é…ç½® (logx-engine-processor/application.yml)

```yaml
server:
  port: 10250

spring:
  application:
    name: logx-engine-processor

  # Kafkaæ¶ˆè´¹
  kafka:
    bootstrap-servers: localhost:29092
    consumer:
      group-id: logx-processor-group
      auto-offset-reset: earliest
      enable-auto-commit: false
      max-poll-records: 500
      concurrency: 3

  # Elasticsearchå†™å…¥
  data:
    elasticsearch:
      uris: http://localhost:9200
      username: elastic
      password: 8rc3Jl1jlAK3uVZZyhF4

# æ‰¹é‡å†™å…¥é…ç½®
logx:
  storage:
    bulk:
      size: 1000
      flush-interval: 5m
      concurrent-requests: 2

logging:
  level:
    com.domidodo.logx: DEBUG
  file:
    name: logs/processor.log
```

---

### 4. ç®¡ç†æ§åˆ¶å°é…ç½® (logx-console-api/application.yml)

```yaml
server:
  port: 8083

spring:
  application:
    name: logx-console-api

  # æ•°æ®æº
  datasource:
    driver-class-name: com.mysql.cj.jdbc.Driver
    url: jdbc:mysql://localhost:3307/logx?useUnicode=true&characterEncoding=utf8
    username: root
    password: root123

  # ElasticsearchæŸ¥è¯¢
  
elasticsearch:
  uris: http://localhost:9200
  username: elastic
  password: 8rc3Jl1jlAK3uVZZyhF4

  # Redisç¼“å­˜
  data:
    redis:
      host: localhost
      port: 6379
      password: redis123

# APIæ–‡æ¡£
knife4j:
  enable: true

logging:
  level:
    com.domidodo.logx: DEBUG
  file:
    name: logs/console-api.log
```

---

## ä¸­é—´ä»¶é…ç½®è¯¦è§£

### 1. MySQL æ•°æ®åº“é…ç½®

**ç”¨é€”**: å­˜å‚¨ç§Ÿæˆ·ã€ç³»ç»Ÿã€è§„åˆ™ç­‰å…ƒæ•°æ®

**è¿æ¥ä¿¡æ¯**:
```
Host: localhost
Port: 3307 (å®¹å™¨æ˜ å°„ï¼Œé¿å…å†²çª)
Database: logx
User: root
Password: root123
```

**åˆå§‹åŒ–è„šæœ¬**: `scripts/init.sql`

**æ•°æ®è¡¨**:
- `sys_tenant` - ç§Ÿæˆ·è¡¨
- `sys_system` - ç³»ç»Ÿè¡¨
- `log_exception_rule` - å¼‚å¸¸è§„åˆ™è¡¨
- `log_notification_config` - é€šçŸ¥é…ç½®è¡¨
- `log_alert_record` - å‘Šè­¦è®°å½•è¡¨

---

### 2. Elasticsearch é…ç½®è¯¦è§£

#### è¿æ¥é…ç½®

```yaml
spring:
  data:
    elasticsearch:
      uris: http://localhost:9200
      username: elastic
      password: 8rc3Jl1jlAK3uVZZyhF4  # é¦–æ¬¡å¯åŠ¨è‡ªåŠ¨ç”Ÿæˆ
      connection-timeout: 10000
      socket-timeout: 30000
```

#### ç´¢å¼•å‘½åè§„åˆ™

```
æ ¼å¼: {prefix}-{tenantId}-{systemId}-{date}
ç¤ºä¾‹: logx-logs-company_a-erp_system-2024.12.27
```

#### åˆ†ç‰‡ä¸å‰¯æœ¬ç­–ç•¥

| ç¯å¢ƒ | ä¸»åˆ†ç‰‡ | å‰¯æœ¬æ•° | è¯´æ˜ |
|------|--------|--------|------|
| å¼€å‘ | 1 | 0 | å•èŠ‚ç‚¹ï¼Œæ— å‰¯æœ¬ |
| æµ‹è¯• | 3 | 1 | ä¸­ç­‰æ•°æ®é‡ |
| ç”Ÿäº§ | 5 | 1-2 | å¤§æ•°æ®é‡ï¼Œé«˜å¯ç”¨ |

#### ç”Ÿå‘½å‘¨æœŸç®¡ç†

```
çƒ­æ•°æ® (Hot)   â†’ 7å¤©  â†’ é«˜æ€§èƒ½SSDï¼Œé¢‘ç¹è¯»å†™
  â†“
æ¸©æ•°æ® (Warm)  â†’ 30å¤© â†’ æ™®é€šç£ç›˜ï¼Œåªè¯»æŸ¥è¯¢
  â†“
å†·æ•°æ® (Cold)  â†’ 90å¤© â†’ å½’æ¡£å­˜å‚¨(MinIO)ï¼Œæå°‘è®¿é—®
  â†“
åˆ é™¤ (Delete)  â†’ è¿‡æœŸåè‡ªåŠ¨æ¸…ç†
```

---

### 3. Kafka é…ç½®è¯¦è§£

#### Topic è§„åˆ’

| Topic | åˆ†åŒºæ•° | å‰¯æœ¬æ•° | ç”¨é€” |
|-------|--------|--------|------|
| logx-logs | 3 | 1 | æ—¥å¿—æ•°æ®æµ |
| logx-alerts | 1 | 1 | å‘Šè­¦é€šçŸ¥ |

#### ç”Ÿäº§è€…ä¼˜åŒ–

```yaml
spring:
  kafka:
    producer:
      acks: 1                    # æƒè¡¡æ€§èƒ½ä¸å¯é æ€§
      batch-size: 16384          # 16KBæ‰¹é‡
      linger-ms: 10              # 10mså»¶è¿Ÿæ‰¹é‡å‘é€
      compression-type: lz4      # LZ4å‹ç¼© (é€Ÿåº¦å¿«)
```

**æ€§èƒ½å¯¹æ¯”**:
- `acks=0`: ä¸ç­‰å¾…ç¡®è®¤ (æœ€å¿«ï¼Œå¯èƒ½ä¸¢å¤±)
- `acks=1`: Leaderç¡®è®¤ (å‡è¡¡ï¼Œæ¨è)
- `acks=all`: æ‰€æœ‰å‰¯æœ¬ç¡®è®¤ (æœ€æ…¢ï¼Œæœ€å¯é )

#### æ¶ˆè´¹è€…ä¼˜åŒ–

```yaml
spring:
  kafka:
    consumer:
      max-poll-records: 500      # å•æ¬¡æ‹‰å–500æ¡
      fetch-min-size: 1024       # æœ€å°1KBæ‰è¿”å›
      concurrency: 3             # 3ä¸ªå¹¶å‘æ¶ˆè´¹è€…
```

---

### 4. Redis é…ç½®è¯¦è§£

#### ç”¨é€”è¯´æ˜

| ç”¨é€” | Key å‰ç¼€ | TTL | è¯´æ˜ |
|------|----------|-----|------|
| é™æµ | `rate_limit:` | 1s-1m | æ»‘åŠ¨çª—å£è®¡æ•° |
| ç¼“å­˜ | `cache:` | 1h | çƒ­ç‚¹æ•°æ®ç¼“å­˜ |
| åˆ†å¸ƒå¼é” | `lock:` | 30s | é˜²æ­¢å¹¶å‘å†²çª |

#### è¿æ¥æ± é…ç½®

```yaml
spring:
  data:
    redis:
      lettuce:
        pool:
          max-active: 8          # æœ€å¤§è¿æ¥æ•°
          max-idle: 8            # æœ€å¤§ç©ºé—²è¿æ¥
          min-idle: 2            # æœ€å°ç©ºé—²è¿æ¥
          max-wait: -1ms         # æ— é™ç­‰å¾…
```

---

### 5. MinIO é…ç½®è¯¦è§£

#### è®¿é—®ä¿¡æ¯

```
Console: http://localhost:9001
Access Key: admin
Secret Key: admin123
```

#### Bucket è®¾ç½®

```yaml
minio:
  bucket-name: logx-archive
  region: us-east-1
```

#### è‡ªåŠ¨åˆ›å»º Bucket

åœ¨ `MinioConfig` ä¸­æ·»åŠ åˆå§‹åŒ–é€»è¾‘ï¼š

```java
@PostConstruct
public void initBucket() throws Exception {
    boolean exists = minioClient.bucketExists(
        BucketExistsArgs.builder().bucket(bucketName).build()
    );
    
    if (!exists) {
        minioClient.makeBucket(
            MakeBucketArgs.builder().bucket(bucketName).build()
        );
        log.info("åˆ›å»ºBucket: {}", bucketName);
    }
}
```

---

## Elasticsearch ç´¢å¼•è®¾è®¡

### 1. ç´¢å¼•æ¨¡æ¿

ç´¢å¼•æ¨¡æ¿åœ¨åº”ç”¨å¯åŠ¨æ—¶è‡ªåŠ¨åˆ›å»º (`EsTemplateManager.java`)ï¼š

```json
{
  "index_patterns": ["logx-logs-*"],
  "template": {
    "settings": {
      "number_of_shards": 5,
      "number_of_replicas": 1,
      "refresh_interval": "5s",
      "codec": "best_compression"
    },
    "mappings": {
      "properties": {
        "traceId": { "type": "keyword" },
        "spanId": { "type": "keyword" },
        "tenantId": { "type": "keyword" },
        "systemId": { "type": "keyword" },
        "timestamp": { "type": "date" },
        "level": { "type": "keyword" },
        "message": {
          "type": "text",
          "analyzer": "ik_max_word",
          "fields": {
            "keyword": { "type": "keyword", "ignore_above": 256 }
          }
        },
        "exception": { "type": "text" },
        "userId": { "type": "keyword" },
        "userName": { "type": "keyword" },
        "className": { "type": "keyword" },
        "methodName": { "type": "keyword" },
        "lineNumber": { "type": "integer" },
        "requestUrl": { "type": "keyword" },
        "requestMethod": { "type": "keyword" },
        "responseTime": { "type": "long" },
        "ip": { "type": "ip" },
        "tags": { "type": "keyword" },
        "extra": { "type": "object", "enabled": false }
      }
    }
  }
}
```

### 2. å­—æ®µè¯´æ˜

| å­—æ®µ | ç±»å‹ | ç´¢å¼• | è¯´æ˜ |
|------|------|------|------|
| traceId | keyword | âœ… | å…¨é“¾è·¯è¿½è¸ªID |
| timestamp | date | âœ… | æ—¥å¿—æ—¶é—´æˆ³ (æ ¸å¿ƒæŸ¥è¯¢å­—æ®µ) |
| level | keyword | âœ… | æ—¥å¿—çº§åˆ« (å¸¸ç”¨è¿‡æ»¤) |
| message | text | âœ… | æ—¥å¿—å†…å®¹ (å…¨æ–‡æ£€ç´¢) |
| exception | text | âœ… | å¼‚å¸¸å †æ ˆ |
| ip | ip | âœ… | IPåœ°å€ (æ”¯æŒèŒƒå›´æŸ¥è¯¢) |
| extra | object | âŒ | æ‰©å±•å­—æ®µ (ä»…å­˜å‚¨ä¸ç´¢å¼•) |

### 3. æŸ¥è¯¢ç¤ºä¾‹

#### æŒ‰æ—¶é—´èŒƒå›´æŸ¥è¯¢

```java
SearchRequest request = SearchRequest.of(s -> s
    .index("logx-logs-*")
    .query(q -> q
        .bool(b -> b
            .filter(f -> f.range(r -> r
                .field("timestamp")
                .gte(JsonData.of(startTime))
                .lte(JsonData.of(endTime))
            ))
        )
    )
    .sort(sort -> sort.field(f -> f.field("timestamp").order(SortOrder.Desc)))
);
```

#### å…¨æ–‡æ£€ç´¢

```java
SearchRequest request = SearchRequest.of(s -> s
    .index("logx-logs-*")
    .query(q -> q
        .match(m -> m
            .field("message")
            .query(keyword)
        )
    )
);
```

#### èšåˆç»Ÿè®¡

```java
SearchRequest request = SearchRequest.of(s -> s
    .index("logx-logs-*")
    .size(0)
    .aggregations("level_count", a -> a
        .terms(t -> t.field("level"))
    )
);
```

---

## gRPC åè®®è¯´æ˜

### 1. Protocol Buffers å®šä¹‰

#### log_service.proto

```protobuf
syntax = "proto3";
import "google/protobuf/struct.proto";

package logx;

service LogService {
  // æ‰¹é‡å‘é€æ—¥å¿—
  rpc SendLogs(LogBatchRequest) returns (LogBatchResponse);
  
  // æµå¼å‘é€æ—¥å¿—
  rpc StreamLogs(stream LogEntry) returns (LogBatchResponse);
}

message LogBatchRequest {
  string tenant_id = 1;
  string system_id = 2;
  string api_key = 3;
  repeated LogEntry logs = 4;
}

message LogEntry {
  string trace_id = 1;
  string tenant_id = 3;
  string system_id = 4;
  int64 timestamp = 5;
  string level = 6;
  string message = 12;
  google.protobuf.Struct extra = 25;  // æ‰©å±•å­—æ®µ
  // ... å…¶ä»–å­—æ®µçœç•¥
}
```

### 2. ä½¿ç”¨ gRPC çš„ä¼˜åŠ¿

| ç‰¹æ€§ | HTTP/JSON | gRPC/Protobuf |
|------|-----------|---------------|
| æ€§èƒ½ | è¾ƒæ…¢ | å¿« 2-5 å€ |
| æ•°æ®å¤§å° | è¾ƒå¤§ | å° 30-50% |
| ç±»å‹å®‰å…¨ | âŒ | âœ… |
| æµå¼ä¼ è¾“ | æœ‰é™ | åŸç”Ÿæ”¯æŒ |
| æµè§ˆå™¨æ”¯æŒ | âœ… | éœ€è¦ gRPC-Web |

### 3. SDK ä½¿ç”¨ gRPC

#### é…ç½®

```yaml
logx:
  mode: grpc
  gateway:
    grpc-host: localhost
    grpc-port: 10241
```

#### ä»£ç ç¤ºä¾‹

```java
LogXClient client = LogXClient.builder()
    .tenantId("company_a")
    .systemId("erp_system")
    .apiKey("sk_test_key_001")
    .grpcEndpoint("localhost", 10241)
    .build();

client.info("æµ‹è¯•gRPCæ—¥å¿—");
```

### 4. æ€§èƒ½å¯¹æ¯”æµ‹è¯•

æµ‹è¯•æ¡ä»¶: 10ä¸‡æ¡æ—¥å¿—ï¼Œæ¯æ¡500å­—èŠ‚

| æ¨¡å¼ | è€—æ—¶ | ååé‡ | ç½‘ç»œæµé‡ |
|------|------|--------|----------|
| HTTP | 15.2s | 6,578/s | 48MB |
| gRPC | 6.8s | 14,705/s | 32MB |

**ç»“è®º**: gRPC é€Ÿåº¦æå‡ 2.2å€ï¼Œæµé‡å‡å°‘ 33%

---

## ä¸€é”®éƒ¨ç½²è„šæœ¬

### 1. å®Œæ•´å¯åŠ¨è„šæœ¬

åˆ›å»º `scripts/start-all.sh`:

```bash
#!/bin/bash

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

echo -e "${GREEN}=== LogX ä¸€é”®éƒ¨ç½²è„šæœ¬ ===${NC}"

# 1. æ£€æŸ¥ç¯å¢ƒ
echo -e "${YELLOW}[1/6] æ£€æŸ¥ç¯å¢ƒ...${NC}"

# æ£€æŸ¥Docker
if ! command -v docker &> /dev/null; then
    echo -e "${RED}âŒ Dockeræœªå®‰è£…${NC}"
    exit 1
fi

# æ£€æŸ¥Docker Compose
if ! command -v docker-compose &> /dev/null; then
    echo -e "${RED}âŒ Docker Composeæœªå®‰è£…${NC}"
    exit 1
fi

# æ£€æŸ¥JDK
if ! command -v java &> /dev/null; then
    echo -e "${RED}âŒ JDKæœªå®‰è£…${NC}"
    exit 1
else
    java_version=$(java -version 2>&1 | awk -F '"' '/version/ {print $2}' | cut -d'.' -f1)
    if [ "$java_version" -lt 17 ]; then
        echo -e "${RED}âŒ JDKç‰ˆæœ¬è¿‡ä½,éœ€è¦JDK 17+${NC}"
        exit 1
    fi
fi

# æ£€æŸ¥Maven
if ! command -v mvn &> /dev/null; then
    echo -e "${RED}âŒ Mavenæœªå®‰è£…${NC}"
    exit 1
fi

echo -e "${GREEN}âœ… ç¯å¢ƒæ£€æŸ¥é€šè¿‡${NC}"

# 2. å¯åŠ¨ä¸­é—´ä»¶
echo -e "${YELLOW}[2/6] å¯åŠ¨ä¸­é—´ä»¶ (MySQL, Redis, ES, Kafka, MinIO)...${NC}"
docker-compose up -d

echo "ç­‰å¾…ä¸­é—´ä»¶å°±ç»ª..."
sleep 30

# æ£€æŸ¥ä¸­é—´ä»¶çŠ¶æ€
if ! docker ps | grep -q logx-mysql; then
    echo -e "${RED}âŒ MySQLå¯åŠ¨å¤±è´¥${NC}"
    exit 1
fi

echo -e "${GREEN}âœ… ä¸­é—´ä»¶å¯åŠ¨æˆåŠŸ${NC}"

# 3. åˆå§‹åŒ–æ•°æ®åº“
echo -e "${YELLOW}[3/6] åˆå§‹åŒ–æ•°æ®åº“...${NC}"
docker exec -i logx-mysql mysql -uroot -proot123 < scripts/init.sql
echo -e "${GREEN}âœ… æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ${NC}"

# 4. ç¼–è¯‘é¡¹ç›®
echo -e "${YELLOW}[4/6] ç¼–è¯‘é¡¹ç›®...${NC}"
mvn clean package -DskipTests
if [ $? -ne 0 ]; then
    echo -e "${RED}âŒ ç¼–è¯‘å¤±è´¥${NC}"
    exit 1
fi
echo -e "${GREEN}âœ… ç¼–è¯‘å®Œæˆ${NC}"

# 5. åˆ›å»ºæ—¥å¿—ç›®å½•
echo -e "${YELLOW}[5/6] åˆ›å»ºæ—¥å¿—ç›®å½•...${NC}"
mkdir -p logs

# 6. å¯åŠ¨æœåŠ¡
echo -e "${YELLOW}[6/6] å¯åŠ¨LogXæœåŠ¡...${NC}"

# é€‰æ‹©éƒ¨ç½²æ¨¡å¼
read -p "é€‰æ‹©éƒ¨ç½²æ¨¡å¼ [1=å•ä½“, 2=å¾®æœåŠ¡]: " mode

if [ "$mode" == "1" ]; then
    # å•ä½“æ¨¡å¼
    echo "å¯åŠ¨å•ä½“åº”ç”¨..."
    nohup java -jar logx-standalone/target/logx-standalone-0.0.1-SNAPSHOT.jar \
        > logs/standalone.log 2>&1 &
    echo $! > logs/standalone.pid
    echo -e "${GREEN}âœ… å•ä½“åº”ç”¨å¯åŠ¨ä¸­...${NC}"
    
else
    # å¾®æœåŠ¡æ¨¡å¼
    echo "å¯åŠ¨å¾®æœåŠ¡..."
    
    # HTTPç½‘å…³
    nohup java -jar logx-gateway/logx-gateway-http/target/logx-gateway-http-0.0.1-SNAPSHOT.jar \
        > logs/gateway-http.log 2>&1 &
    echo $! > logs/gateway-http.pid
    
    # æ—¥å¿—å¤„ç†å™¨
    nohup java -jar logx-engine/logx-engine-processor/target/logx-engine-processor-0.0.1-SNAPSHOT.jar \
        > logs/processor.log 2>&1 &
    echo $! > logs/processor.pid
    
    # å¼‚å¸¸æ£€æµ‹
    nohup java -jar logx-engine/logx-engine-detection/target/logx-engine-detection-0.0.1-SNAPSHOT.jar \
        > logs/detection.log 2>&1 &
    echo $! > logs/detection.pid
    
    # å­˜å‚¨ç®¡ç†
    nohup java -jar logx-engine/logx-engine-storage/target/logx-engine-storage-0.0.1-SNAPSHOT.jar \
        > logs/storage.log 2>&1 &
    echo $! > logs/storage.pid
    
    # ç®¡ç†æ§åˆ¶å°
    nohup java -jar logx-console/logx-console-api/target/logx-console-api-0.0.1-SNAPSHOT.jar \
        > logs/console-api.log 2>&1 &
    echo $! > logs/console-api.pid
    
    echo -e "${GREEN}âœ… å¾®æœåŠ¡å¯åŠ¨ä¸­...${NC}"
fi

echo ""
echo -e "${GREEN}=== éƒ¨ç½²å®Œæˆ! ===${NC}"
echo ""
echo "æœåŠ¡åœ°å€:"
echo "  - HTTPç½‘å…³:    http://localhost:10240"
echo "  - ç®¡ç†æ§åˆ¶å°:   http://localhost:8083"
echo "  - APIæ–‡æ¡£:     http://localhost:8083/doc.html"
echo "  - Kibana:      http://localhost:5601"
echo "  - MinIO:       http://localhost:9001"
echo ""
echo "æŸ¥çœ‹æ—¥å¿—:"
echo "  tail -f logs/*.log"
echo ""
```

### 2. åœæ­¢è„šæœ¬

åˆ›å»º `scripts/stop-all.sh`:

```bash
#!/bin/bash

echo "åœæ­¢LogXæœåŠ¡..."

# è¯»å–PIDå¹¶åœæ­¢
for pidfile in logs/*.pid; do
    if [ -f "$pidfile" ]; then
        pid=$(cat "$pidfile")
        if ps -p $pid > /dev/null; then
            echo "åœæ­¢è¿›ç¨‹ $pid..."
            kill $pid
        fi
        rm -f "$pidfile"
    fi
done

echo "åœæ­¢ä¸­é—´ä»¶..."
docker-compose down

echo "âœ… å…¨éƒ¨åœæ­¢å®Œæˆ"
```

### 3. èµ‹äºˆæ‰§è¡Œæƒé™

```bash
chmod +x scripts/start-all.sh
chmod +x scripts/stop-all.sh
```

---

## ç›‘æ§ä¸è¿ç»´

### 1. å¥åº·æ£€æŸ¥

#### æ£€æŸ¥è„šæœ¬

åˆ›å»º `scripts/health-check.sh`:

```bash
#!/bin/bash

echo "=== LogX å¥åº·æ£€æŸ¥ ==="

# æ£€æŸ¥ä¸­é—´ä»¶
echo ""
echo "ä¸­é—´ä»¶çŠ¶æ€:"
docker ps --filter "name=logx-" --format "table {{.Names}}\t{{.Status}}"

# æ£€æŸ¥åº”ç”¨ç«¯å£
echo ""
echo "åº”ç”¨ç«¯å£:"
for port in 8080 10240 10250 8083; do
    if nc -z localhost $port 2>/dev/null; then
        echo "âœ… ç«¯å£ $port: æ­£å¸¸"
    else
        echo "âŒ ç«¯å£ $port: å¼‚å¸¸"
    fi
done

# æ£€æŸ¥ESå¥åº·
echo ""
echo "Elasticsearch:"
curl -s http://localhost:9200/_cluster/health | jq .

# æ£€æŸ¥Kafka
echo ""
echo "Kafka Topics:"
docker exec logx-kafka kafka-topics.sh \
    --bootstrap-server localhost:9092 --list
```

### 2. æ€§èƒ½ç›‘æ§æŒ‡æ ‡

#### å…³é”®æŒ‡æ ‡

| æŒ‡æ ‡ç±»å‹ | æŒ‡æ ‡åç§° | å‘Šè­¦é˜ˆå€¼ | è¯´æ˜ |
|---------|---------|---------|------|
| QPS | æ—¥å¿—å†™å…¥QPS | >10000 | ç½‘å…³ååé‡ |
| å»¶è¿Ÿ | ESå†™å…¥å»¶è¿Ÿ | >100ms | å†™å…¥æ€§èƒ½ |
| é˜Ÿåˆ— | Kafkaæ¶ˆæ¯å †ç§¯ | >10000 | æ¶ˆè´¹èƒ½åŠ› |
| èµ„æº | ESå †å†…å­˜ä½¿ç”¨ç‡ | >80% | å†…å­˜å‹åŠ› |

#### Prometheus é›†æˆ

åœ¨ `application.yml` æ·»åŠ :

```yaml
management:
  endpoints:
    web:
      exposure:
        include: health,info,metrics,prometheus
  metrics:
    export:
      prometheus:
        enabled: true
```

### 3. æ—¥å¿—è½®è½¬

ä½¿ç”¨ logback é…ç½®:

```xml
<configuration>
    <appender name="FILE" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <file>logs/logx.log</file>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>logs/logx.%d{yyyy-MM-dd}.%i.log.gz</fileNamePattern>
            <timeBasedFileNamingAndTriggeringPolicy 
                class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP">
                <maxFileSize>100MB</maxFileSize>
            </timeBasedFileNamingAndTriggeringPolicy>
            <maxHistory>30</maxHistory>
        </rollingPolicy>
        <encoder>
            <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{50} - %msg%n</pattern>
        </encoder>
    </appender>
    
    <root level="INFO">
        <appender-ref ref="FILE" />
    </root>
</configuration>
```

---

## å¸¸è§é—®é¢˜æ’æŸ¥

### 1. ES è¿æ¥å¤±è´¥

**ç°è±¡**: `ElasticsearchException: Connection refused`

**æ’æŸ¥æ­¥éª¤**:
```bash
# 1. æ£€æŸ¥ESæ˜¯å¦å¯åŠ¨
docker ps | grep elasticsearch

# 2. æŸ¥çœ‹ESæ—¥å¿—
docker logs logx-es

# 3. æµ‹è¯•è¿æ¥
curl http://localhost:9200

# 4. é‡ç½®å¯†ç 
docker exec -it logx-es elasticsearch-reset-password -u elastic
```

### 2. Kafka æ¶ˆè´¹å †ç§¯

**ç°è±¡**: æ—¥å¿—å»¶è¿Ÿï¼Œ`lag` å€¼å¾ˆå¤§

**è§£å†³æ–¹æ¡ˆ**:
```yaml
# å¢åŠ æ¶ˆè´¹è€…å¹¶å‘æ•°
spring:
  kafka:
    consumer:
      concurrency: 5  # ä»3å¢åŠ åˆ°5
      max-poll-records: 1000  # å¢åŠ æ‰¹é‡å¤§å°
```

### 3. å†…å­˜æº¢å‡º

**ç°è±¡**: `OutOfMemoryError: Java heap space`

**è§£å†³æ–¹æ¡ˆ**:
```bash
# å¢åŠ JVMå †å†…å­˜
java -Xms2g -Xmx4g -jar app.jar

# æˆ–åœ¨å¯åŠ¨è„šæœ¬ä¸­è®¾ç½®
export JAVA_OPTS="-Xms2g -Xmx4g"
```

---

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. Elasticsearch ä¼˜åŒ–

```yaml
logx:
  storage:
    index:
      shards: 3              # å‡å°‘åˆ†ç‰‡æ•° (æ•°æ®é‡<500GB)
      refresh-interval: 30s  # å¢åŠ åˆ·æ–°é—´éš” (é™ä½å†™å…¥å‹åŠ›)
    bulk:
      size: 5000             # å¢åŠ æ‰¹é‡å¤§å°
      concurrent-requests: 4 # å¢åŠ å¹¶å‘å†™å…¥
```

### 2. Kafka ä¼˜åŒ–

```yaml
spring:
  kafka:
    producer:
      batch-size: 32768      # å¢åŠ åˆ°32KB
      linger-ms: 20          # å»¶è¿Ÿ20msæ‰¹é‡å‘é€
      compression-type: zstd # ä½¿ç”¨zstdå‹ç¼© (å‹ç¼©ç‡æ›´é«˜)
```

### 3. åº”ç”¨å±‚ä¼˜åŒ–

- ä½¿ç”¨ç¼“å†²åŒºå‡å°‘ç½‘ç»œè¯·æ±‚
- å¼‚æ­¥å‘é€æ—¥å¿—
- åˆç†è®¾ç½®è¿æ¥æ± å¤§å°
- å¯ç”¨æ—¥å¿—å‹ç¼©

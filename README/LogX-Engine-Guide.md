# LogX Engine æ¨¡å—æŠ€æœ¯æ–‡æ¡£

## ğŸ“‘ ç›®å½•

- [æ¨¡å—æ¦‚è¿°](#æ¨¡å—æ¦‚è¿°)
- [æ—¥å¿—å¤„ç†æµç¨‹](#æ—¥å¿—å¤„ç†æµç¨‹)
- [æ ¸å¿ƒç»„ä»¶](#æ ¸å¿ƒç»„ä»¶)
- [æ•°æ®è„±æ•](#æ•°æ®è„±æ•)
- [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
- [ç›‘æ§æŒ‡æ ‡](#ç›‘æ§æŒ‡æ ‡)

---

## æ¨¡å—æ¦‚è¿°

### æ¶æ„å›¾

```
SDK â†’ Gateway â†’ Kafka(logx-logs) â†’ Processor â†’ Elasticsearch
                                        â†“
                                   Kafka(logx-logs-processing) â†’ Detection
```

### å¤„ç†æµç¨‹

```
1. Kafka Consumer æ‰¹é‡æ‹‰å–æ—¥å¿— (500æ¡/æ‰¹)
2. LogParser è§£æ + è„±æ• + æ ‡å‡†åŒ–
3. ElasticsearchWriter æ‰¹é‡å†™å…¥
4. è½¬å‘åˆ° Detection æ¨¡å— (å¼‚å¸¸æ£€æµ‹)
5. æäº¤ Offset
```

### æ¨¡å—ç»„æˆ

```
logx-engine/
â”œâ”€â”€ logx-engine-processor/      # æ—¥å¿—å¤„ç†å™¨ (æ ¸å¿ƒ)
â”‚   â”œâ”€â”€ consumer/
â”‚   â”‚   â””â”€â”€ LogKafkaConsumer.java      # Kafkaæ¶ˆè´¹è€…
â”‚   â”œâ”€â”€ parser/
â”‚   â”‚   â””â”€â”€ LogParser.java             # æ—¥å¿—è§£æå™¨
â”‚   â””â”€â”€ writer/
â”‚       â””â”€â”€ ElasticsearchWriter.java   # ESå†™å…¥å™¨
â”‚
â”œâ”€â”€ logx-engine-storage/        # å­˜å‚¨ç®¡ç†
â”‚   â”œâ”€â”€ elasticsearch/
â”‚   â”‚   â”œâ”€â”€ EsIndexManager.java        # ç´¢å¼•ç®¡ç†
â”‚   â”‚   â””â”€â”€ EsTemplateManager.java     # æ¨¡æ¿ç®¡ç†
â”‚   â”œâ”€â”€ lifecycle/              # ç”Ÿå‘½å‘¨æœŸç®¡ç†
â”‚   â””â”€â”€ archive/                # å½’æ¡£æœåŠ¡
â”‚
â””â”€â”€ logx-engine-detection/      # å¼‚å¸¸æ£€æµ‹
    â”œâ”€â”€ rule/                   # è§„åˆ™å¼•æ“
    â”œâ”€â”€ analyzer/               # åˆ†æå™¨
    â””â”€â”€ alert/                  # å‘Šè­¦è§¦å‘
```

---

## æ—¥å¿—å¤„ç†æµç¨‹

### 1. Kafka æ¶ˆè´¹

**LogKafkaConsumer.java** - æ‰¹é‡æ¶ˆè´¹æ—¥å¿—

#### é…ç½®

```yaml
spring:
  kafka:
    bootstrap-servers: localhost:29092
    consumer:
      group-id: logx-processor-group
      auto-offset-reset: latest
      enable-auto-commit: false    # æ‰‹åŠ¨æäº¤
      max-poll-records: 500        # æ‰¹é‡å¤§å°
      concurrency: 3               # å¹¶å‘æ¶ˆè´¹è€…

logx:
  kafka:
    topic:
      log-ingestion: logx-logs              # è¾“å…¥Topic
      log-processing: logx-logs-processing  # è¾“å‡ºTopic(ç»™Detection)
      dead-letter: logx-logs-dlq            # æ­»ä¿¡é˜Ÿåˆ—
  consumer:
    max-retries: 3                 # æœ€å¤§é‡è¯•æ¬¡æ•°
    retry-backoff-ms: 1000         # é‡è¯•é—´éš”
```

#### æ ¸å¿ƒä»£ç 

```java

@KafkaListener(
        topics = "${logx.kafka.topic.log-ingestion:logx-logs}",
        groupId = "${spring.kafka.consumer.group-id:logx-processor-group}",
        containerFactory = "kafkaListenerContainerFactory"
)
public void consumeLogs(List<String> messages, Acknowledgment acknowledgment) {
    // 1. æ‰¹é‡è§£ææ—¥å¿—
    ParseResult parseResult = parseMessages(messages);

    // 2. æ‰¹é‡å†™å…¥ ES (å¸¦é‡è¯•)
    boolean writeSuccess = writeWithRetry(parseResult.validLogs);

    // 3. è½¬å‘åˆ° Detection æ¨¡å—
    boolean forwardSuccess = forwardToDetection(parseResult.validLogs);

    // 4. å…¨éƒ¨æˆåŠŸæ‰æäº¤ offset
    if (writeSuccess && forwardSuccess) {
        acknowledgment.acknowledge();
    } else {
        // å¤±è´¥çš„å‘é€åˆ°æ­»ä¿¡é˜Ÿåˆ—
        sendToDeadLetterQueue(messages, "å†™å…¥ESæˆ–è½¬å‘å¤±è´¥");
        acknowledgment.acknowledge();  // é¿å…é˜»å¡
    }
}
```

#### å…³é”®ç‰¹æ€§

| ç‰¹æ€§       | è¯´æ˜           | å®ç°                         |
|----------|--------------|----------------------------|
| **æ‰¹é‡æ¶ˆè´¹** | 500æ¡/æ‰¹       | `max-poll-records=500`     |
| **æ‰‹åŠ¨æäº¤** | å¤„ç†æˆåŠŸæ‰æäº¤      | `enable-auto-commit=false` |
| **é‡è¯•æœºåˆ¶** | æŒ‡æ•°é€€é¿         | 1s â†’ 2s â†’ 4s               |
| **æ­»ä¿¡é˜Ÿåˆ—** | å¤±è´¥æ¶ˆæ¯ä¿å­˜       | `logx-logs-dlq`            |
| **è½¬å‘æœºåˆ¶** | å‘é€åˆ°Detection | `logx-logs-processing`     |

---

### 2. æ—¥å¿—è§£æ

**LogParser.java** - è§£æã€æ ‡å‡†åŒ–ã€è„±æ•

#### å¤„ç†æ­¥éª¤

```java
public Map<String, Object> parse(String logJson) {
    // 1. JSON è§£æ
    Map<String, Object> logMap = JsonUtil.parseObject(logJson);

    // 2. æ ‡å‡†åŒ–å¤„ç†
    Map<String, Object> normalized = normalize(logMap);

    // 3. æ•æ„Ÿä¿¡æ¯è„±æ•
    desensitizeEnhanced(normalized);

    // 4. å­—æ®µè¡¥å…¨
    fillMissingFields(normalized);

    // 5. å­—æ®µéªŒè¯
    validateFields(normalized);

    return normalized;
}
```

#### å­—æ®µæ ‡å‡†åŒ–

æ”¯æŒå¤šç§å­—æ®µåå˜ä½“ï¼š

```java
// ç¤ºä¾‹ï¼šæ”¯æŒé©¼å³°å’Œä¸‹åˆ’çº¿
normalized.put("traceId",getString(logMap, "traceId","trace_id"));
        normalized.

put("className",getString(logMap, "className","class_name"));
        normalized.

put("requestUrl",getString(logMap, "requestUrl","request_url","url"));
```

**æ”¯æŒçš„å­—æ®µåˆ«å**:

| æ ‡å‡†å­—æ®µ          | æ”¯æŒçš„åˆ«å                   |
|---------------|-------------------------|
| traceId       | trace_id                |
| spanId        | span_id                 |
| className     | class_name              |
| methodName    | method_name             |
| lineNumber    | line_number             |
| requestUrl    | request_url, url        |
| requestMethod | request_method, method  |
| responseTime  | response_time, duration |

#### æ—¥å¿—çº§åˆ«æ ‡å‡†åŒ–

```java
private String normalizeLevel(String level) {
    return switch (level.toUpperCase()) {
        case "WARN", "WARNING" -> "WARN";
        case "ERR", "ERROR", "SEVERE" -> "ERROR";
        case "FATAL", "CRITICAL" -> "FATAL";
        case "TRACE", "FINEST" -> "TRACE";
        case "DEBUG", "FINE" -> "DEBUG";
        default -> "INFO";
    };
}
```

#### æ—¶é—´æˆ³è§£æ

æ”¯æŒå¤šç§æ ¼å¼ï¼š

```java
// æ”¯æŒçš„æ—¶é—´æˆ³æ ¼å¼
private static final List<DateTimeFormatter> TIMESTAMP_FORMATTERS = Arrays.asList(
        DateTimeFormatter.ISO_LOCAL_DATE_TIME,           // 2024-12-27T10:30:00
        DateTimeFormatter.ISO_OFFSET_DATE_TIME,          // 2024-12-27T10:30:00+08:00
        DateTimeFormatter.ISO_ZONED_DATE_TIME,           // 2024-12-27T10:30:00+08:00[Asia/Shanghai]
        DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss"),
        DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss.SSS"),
        DateTimeFormatter.ofPattern("yyyy/MM/dd HH:mm:ss")
);

// è‡ªåŠ¨å¤„ç†
-Long:

æ¯«ç§’æ—¶é—´æˆ³(13ä½)
-Integer:

ç§’æ—¶é—´æˆ³(10ä½)
-LocalDateTime:ç›´æ¥ä½¿ç”¨
-String:å°è¯•å¤šç§æ ¼å¼è§£æ
```

---

### 3.æ•°æ®è„±æ•

#### è„±æ•è§„åˆ™

LogParser è‡ªåŠ¨å¯¹æ•æ„Ÿä¿¡æ¯è¿›è¡Œè„±æ•ï¼š

| ç±»å‹      | æ­£åˆ™è¡¨è¾¾å¼             | è„±æ•è§„åˆ™     | ç¤ºä¾‹                  |
|---------|-------------------|----------|---------------------|
| **æ‰‹æœºå·** | `1[3-9]\d{9}`     | ä¿ç•™å‰3å4   | 138****5678         |
| **èº«ä»½è¯** | `\d{17}[\dXx]`    | ä¿ç•™å‰3å4   | 310***********1234  |
| **é‚®ç®±**  | `[^@]+@[^.]+\..+` | ä¿ç•™é¦–å­—ç¬¦å’ŒåŸŸå | u***@example.com    |
| **é“¶è¡Œå¡** | `\d{13,19}`       | ä¿ç•™å‰4å4   | 6222 **** **** 1234 |
| **ç”¨æˆ·å** | -                 | ä¿ç•™ç¬¬ä¸€ä¸ªå­—ç¬¦  | å¼ **                 |

#### è„±æ•å­—æ®µ

```java
// è‡ªåŠ¨è„±æ•çš„å­—æ®µ
-message         // æ—¥å¿—æ¶ˆæ¯
-requestParams   // è¯·æ±‚å‚æ•°
-exception       // å¼‚å¸¸å †æ ˆ
-userName        // ç”¨æˆ·å (ä¿ç•™å§“)
```

#### è„±æ•ç¤ºä¾‹

**åŸå§‹æ—¥å¿—**:

```json
{
  "message": "ç”¨æˆ·ç™»å½•ï¼Œæ‰‹æœºå·ï¼š13812345678ï¼Œé‚®ç®±ï¼šuser@example.com",
  "requestParams": "{\"idCard\":\"310123199001011234\"}",
  "userName": "å¼ ä¸‰ä¸°"
}
```

**è„±æ•å**:

```json
{
  "message": "ç”¨æˆ·ç™»å½•ï¼Œæ‰‹æœºå·ï¼š138****5678ï¼Œé‚®ç®±ï¼šu***@example.com",
  "requestParams": "{\"idCard\":\"310***********1234\"}",
  "userName": "å¼ **"
}
```

#### æ•æ„Ÿå­—æ®µè¿‡æ»¤

```java
// extra å­—æ®µä¸­çš„æ•æ„Ÿ key ä¼šè¢«æ›¿æ¢ä¸º "***"
Set<String> sensitiveKeys = Set.of(
        "password", "pwd", "token", "secret", "key",
        "authorization", "auth", "apiKey", "api_key"
);

// ç¤ºä¾‹
åŸå§‹:{"password":"abc123","amount":100}
è¿‡æ»¤:{"password":"***","amount":100}
```

---

### 4. Elasticsearch å†™å…¥

**ElasticsearchWriter.java** - æ‰¹é‡å†™å…¥ã€è‡ªåŠ¨åˆ›å»ºç´¢å¼•

#### æ ¸å¿ƒç‰¹æ€§

| ç‰¹æ€§       | è¯´æ˜        | é…ç½®             |
|----------|-----------|----------------|
| **æ‰¹é‡å†™å…¥** | å‡å°‘ç½‘ç»œå¼€é”€    | `max-size=500` |
| **è‡ªåŠ¨åˆ†æ‰¹** | è¶…è¿‡é˜ˆå€¼è‡ªåŠ¨åˆ†å‰²  | è‡ªåŠ¨å¤„ç†           |
| **ç´¢å¼•ç¼“å­˜** | é¿å…é‡å¤æ£€æŸ¥    | å†…å­˜ç¼“å­˜           |
| **å¹‚ç­‰å†™å…¥** | ä½¿ç”¨IDé˜²é‡å¤   | traceId+spanId |
| **è‡ªåŠ¨åˆ›å»º** | ç´¢å¼•ä¸å­˜åœ¨è‡ªåŠ¨åˆ›å»º | è‡ªåŠ¨å¤„ç†           |

#### ç´¢å¼•å‘½åè§„åˆ™

```java
// æ ¼å¼: logx-logs-{tenantId}-{systemId}-{yyyy.MM.dd}
generateIndexName(log):
tenantId =

sanitizeIndexComponent(log.get("tenantId"), "default")
systemId =

sanitizeIndexComponent(log.get("systemId"), "unknown")
date =

extractDate(log)  // yyyy.MM.dd
    
    return"logx-logs-"+tenantId +"-"+systemId +"-"+
        date

// ç¤ºä¾‹
                tenantId = company_a, systemId = erp_system, date = 2024.12.27
        â†’logx-logs-company_a-erp_system-2024.12.27
```

#### å®‰å…¨é˜²æŠ¤

```java
// 1. ç´¢å¼•åç§°æ¸…ç† (é˜²æ³¨å…¥)
private String sanitizeIndexComponent(String input, String defaultValue) {
    // è½¬å°å†™
    String sanitized = input.toLowerCase().trim();

    // åªä¿ç•™å­—æ¯ã€æ•°å­—ã€è¿å­—ç¬¦
    sanitized = sanitized.replaceAll("[^a-z0-9-]", "");

    // é™åˆ¶é•¿åº¦
    if (sanitized.length() > 50) {
        sanitized = sanitized.substring(0, 50);
    }

    return sanitized;
}

// 2. ç´¢å¼•åç§°é•¿åº¦é™åˆ¶
MAX_INDEX_NAME_LENGTH =200  // ESé™åˆ¶255

// 3. æ­£åˆ™éªŒè¯
SAFE_NAME_PATTERN ="^[a-z0-9-]+$"
```

#### æ‰¹é‡å†™å…¥æµç¨‹

```java
public int bulkWrite(List<Map<String, Object>> logs) {
    // 1. åˆ†æ‰¹å¤„ç† (500æ¡/æ‰¹)
    List<List<Map<String, Object>>> batches = splitIntoBatches(logs, maxBulkSize);

    for (List<Map<String, Object>> batch : batches) {
        // 2. é¢„å…ˆç¡®ä¿ç´¢å¼•å­˜åœ¨
        Set<String> requiredIndices = extractIndices(batch);
        ensureIndicesExist(requiredIndices);

        // 3. æ„å»ºæ‰¹é‡è¯·æ±‚
        BulkRequest.Builder bulkBuilder = new BulkRequest.Builder();
        for (Map<String, Object> log : batch) {
            String indexName = generateIndexName(log);
            String documentId = extractDocumentId(log);  // å¹‚ç­‰æ€§

            bulkBuilder.operations(op -> op
                    .index(idx -> idx
                            .index(indexName)
                            .id(documentId)
                            .document(log)
                    )
            );
        }

        // 4. æ‰§è¡Œæ‰¹é‡å†™å…¥
        BulkResponse response = elasticsearchClient.bulk(bulkBuilder.build());

        // 5. å¤„ç†ç»“æœ
        processBulkResponse(response, batch.size());
    }
}
```

#### æ–‡æ¡£IDç”Ÿæˆ (å¹‚ç­‰æ€§)

```java
private String extractDocumentId(Map<String, Object> log) {
    // 1. ä¼˜å…ˆä½¿ç”¨æ—¥å¿—ID
    Object id = log.get("id");
    if (id != null) return id.toString();

    // 2. ä½¿ç”¨ traceId + spanId ç»„åˆ (æ¨è)
    String traceId = (String) log.get("traceId");
    String spanId = (String) log.get("spanId");
    if (traceId != null && spanId != null) {
        return traceId + "-" + spanId;
    }

    // 3. æœ€åçš„ fallback
    return System.currentTimeMillis() + "-" + (int) (Math.random() * 10000);
}
```

**å¹‚ç­‰æ€§ä¿è¯**: ç›¸åŒ traceId+spanId çš„æ—¥å¿—ä¸ä¼šé‡å¤æ’å…¥

#### ç´¢å¼•è‡ªåŠ¨åˆ›å»º

```java
private void ensureIndicesExist(Set<String> indexNames) {
    for (String indexName : indexNames) {
        // 1. æ£€æŸ¥ç¼“å­˜
        if (indexExistenceCache.get(indexName)) {
            continue;
        }

        // 2. æ£€æŸ¥ES
        boolean exists = checkIndexExists(indexName);

        // 3. ä¸å­˜åœ¨åˆ™åˆ›å»º
        if (!exists) {
            IndexInfo info = parseIndexName(indexName);
            esIndexManager.createLogIndex(
                    info.tenantId,
                    info.systemId,
                    info.date
            );
        }

        // 4. æ›´æ–°ç¼“å­˜
        indexExistenceCache.put(indexName, true);
    }
}
```

---

### 5. è½¬å‘åˆ° Detection

**ç›®çš„**: å°†æ—¥å¿—è½¬å‘åˆ°å¼‚å¸¸æ£€æµ‹æ¨¡å—

```java
private boolean forwardToDetection(List<Map<String, Object>> logs) {
    List<CompletableFuture<?>> futures = new ArrayList<>();

    for (Map<String, Object> log : logs) {
        String logJson = JsonUtil.toJson(log);
        String key = generateKey(log);  // tenantId:systemId:traceId

        CompletableFuture<?> future = kafkaTemplate.send(
                "logx-logs-processing",  // Topic
                key,                      // Key (åˆ†åŒº)
                logJson                   // Value
        );

        futures.add(future);
    }

    // ç­‰å¾…æ‰€æœ‰å‘é€å®Œæˆ
    CompletableFuture.allOf(futures.toArray(new CompletableFuture[0]))
            .get(30, TimeUnit.SECONDS);

    return allSuccess;
}
```

**Keyç”Ÿæˆè§„åˆ™**: `{tenantId}:{systemId}:{traceId}`

- ä¿è¯ç›¸åŒç§Ÿæˆ·/ç³»ç»Ÿçš„æ—¥å¿—åœ¨åŒä¸€åˆ†åŒº
- ä¾¿äºDetectionæ¨¡å—æŒ‰ç§Ÿæˆ·å¤„ç†

---

## æ ¸å¿ƒç»„ä»¶

### 1. é‡è¯•æœºåˆ¶

#### æŒ‡æ•°é€€é¿

```java
private boolean writeWithRetry(List<Map<String, Object>> logs) {
    int retryCount = 0;

    while (retryCount <= maxRetries) {
        try {
            int successCount = elasticsearchWriter.bulkWrite(logs);
            if (successCount == logs.size()) {
                return true;  // å…¨éƒ¨æˆåŠŸ
            }
        } catch (Exception e) {
            log.error("å†™å…¥ESå¤±è´¥ï¼Œé‡è¯•{}/{}", retryCount, maxRetries);
        }

        // æŒ‡æ•°é€€é¿: 1s â†’ 2s â†’ 4s
        if (retryCount < maxRetries) {
            retryCount++;
            long backoff = retryBackoffMs * (1L << (retryCount - 1));
            long actualBackoff = Math.min(backoff, 10000);  // æœ€å¤š10s
            Thread.sleep(actualBackoff);
        } else {
            break;
        }
    }

    return false;
}
```

**é‡è¯•ç­–ç•¥**:

- æœ€å¤§é‡è¯•æ¬¡æ•°: 3æ¬¡
- é€€é¿é—´éš”: 1s â†’ 2s â†’ 4s
- æœ€å¤§ç­‰å¾…: 10s

---

### 2. æ­»ä¿¡é˜Ÿåˆ—

#### ç”¨é€”

å¤±è´¥çš„æ¶ˆæ¯å‘é€åˆ°æ­»ä¿¡é˜Ÿåˆ—ï¼Œé¿å…ä¸¢å¤±ï¼š

```java
private void sendToDeadLetterQueue(List<String> messages, String reason) {
    for (String message : messages) {
        kafkaTemplate.send(
                "logx-logs-dlq",  // æ­»ä¿¡Topic
                reason,           // Key (å¤±è´¥åŸå› )
                message           // Value (åŸå§‹æ¶ˆæ¯)
        );
    }

    log.info("å·²å°†{}/{}æ¶ˆæ¯å‘é€åˆ°æ­»ä¿¡é˜Ÿåˆ—ï¼š{}",
            successCount, messages.size(), reason);
}
```

**å¤±è´¥åŸå› **:

- è§£æå¤±è´¥
- å†™å…¥ESå¤±è´¥
- è½¬å‘å¤±è´¥
- å¼‚å¸¸é”™è¯¯

**åç»­å¤„ç†**:

- äººå·¥å®¡æŸ¥
- é‡æ–°å¤„ç†
- æ•°æ®åˆ†æ

---

### 3. ç´¢å¼•å­˜åœ¨æ€§ç¼“å­˜

```java
// å†…å­˜ç¼“å­˜ï¼Œé¿å…é¢‘ç¹æ£€æŸ¥ES
private final Map<String, Boolean> indexExistenceCache = new ConcurrentHashMap<>();

// ä½¿ç”¨
if(indexExistenceCache.

getOrDefault(indexName, false)){
        return;  // ç¼“å­˜å‘½ä¸­ï¼Œè·³è¿‡æ£€æŸ¥
        }

boolean exists = checkIndexExists(indexName);
indexExistenceCache.

put(indexName, true);  // æ›´æ–°ç¼“å­˜
```

**ä¼˜ç‚¹**:

- å‡å°‘ESæŸ¥è¯¢
- æé«˜æ€§èƒ½
- çº¿ç¨‹å®‰å…¨ (ConcurrentHashMap)

**ç¼“å­˜å¤±æ•ˆ**:

```java
// 1. å†™å…¥å¤±è´¥æ—¶æ¸…ç©ºç¼“å­˜
if(e.getMessage().

contains("all shards failed")){
        indexExistenceCache.

clear();
}

// 2. æ‰‹åŠ¨æ¸…ç©º
public void clearIndexCache() {
    indexExistenceCache.clear();
}
```

---

## æ€§èƒ½ä¼˜åŒ–

### 1. æ‰¹é‡å¤„ç†

| é˜¶æ®µ      | æ‰¹é‡å¤§å° | è¯´æ˜                      |
|---------|------|-------------------------|
| Kafkaæ¶ˆè´¹ | 500  | `max-poll-records`      |
| ESå†™å…¥    | 500  | `logx.es.bulk.max-size` |
| Kafkaè½¬å‘ | å¼‚æ­¥æ‰¹é‡ | CompletableFuture       |

### 2. å¹¶å‘é…ç½®

```yaml
spring:
  kafka:
    consumer:
      concurrency: 3  # 3ä¸ªå¹¶å‘æ¶ˆè´¹è€…

logx:
  storage:
    bulk:
      concurrent-requests: 2  # 2ä¸ªå¹¶å‘å†™å…¥
```

**ååé‡ä¼°ç®—**:

```
å•æ¶ˆè´¹è€…: 500æ¡/æ¬¡ Ã— 2æ¬¡/ç§’ = 1000æ¡/ç§’
3ä¸ªæ¶ˆè´¹è€…: 1000 Ã— 3 = 3000æ¡/ç§’
```

### 3. å†…å­˜ä¼˜åŒ–

```java
// 1. ä½¿ç”¨å¯¹è±¡æ±  (å¦‚æœéœ€è¦)
// 2. åŠæ—¶é‡Šæ”¾å¤§å¯¹è±¡
List<Map<String, Object>> logs = ...;
        elasticsearchWriter.

bulkWrite(logs);
logs.

clear();  // é‡Šæ”¾å†…å­˜

// 3. é™åˆ¶æ‰¹é‡å¤§å°
MAX_BULK_SIZE =500  // é˜²æ­¢OOM
```

### 4. ç½‘ç»œä¼˜åŒ–

```yaml
# Kafkaä¼˜åŒ–
spring:
  kafka:
    consumer:
      fetch-min-size: 1024      # æœ€å°æ‹‰å–1KB
      fetch-max-wait: 500       # æœ€å¤§ç­‰å¾…500ms

# ESä¼˜åŒ–
logx:
  storage:
    bulk:
      flush-interval: 5m        # 5åˆ†é’Ÿåˆ·æ–°ä¸€æ¬¡
```

---

## ç›‘æ§æŒ‡æ ‡

### 1. Micrometer æŒ‡æ ‡

```java
private void recordMetrics(int successCount, int failCount) {
    // æˆåŠŸè®¡æ•°
    meterRegistry.counter("logx.kafka.consumer.success",
                    "tenant", String.valueOf(TenantContext.getTenantId()))
            .increment(successCount);

    // å¤±è´¥è®¡æ•°
    meterRegistry.counter("logx.kafka.consumer.failed",
                    "tenant", String.valueOf(TenantContext.getTenantId()))
            .increment(failCount);

    // æ‰¹é‡å¤§å°
    meterRegistry.gauge("logx.kafka.consumer.last.batch.size", successCount);
}
```

### 2. å…³é”®æŒ‡æ ‡

| æŒ‡æ ‡                                    | ç±»å‹      | è¯´æ˜     | å‘Šè­¦é˜ˆå€¼   |
|---------------------------------------|---------|--------|--------|
| `logx.kafka.consumer.success`         | Counter | æˆåŠŸå¤„ç†æ•°  | -      |
| `logx.kafka.consumer.failed`          | Counter | å¤±è´¥å¤„ç†æ•°  | >100   |
| `logx.kafka.consumer.last.batch.size` | Gauge   | æœ€è¿‘æ‰¹é‡å¤§å° | -      |
| `logx.kafka.lag`                      | Gauge   | æ¶ˆè´¹å»¶è¿Ÿ   | >10000 |
| `logx.es.write.duration`              | Timer   | å†™å…¥è€—æ—¶   | >100ms |

### 3. æ—¥å¿—ç›‘æ§

```java
// å¤„ç†å®Œæˆæ—¥å¿—
log.info("å·²å¤„ç† {} ä¸ªæ—¥å¿—ï¼š{} ä¸ªæœ‰æ•ˆï¼Œ{} ä¸ªè§£æå¤±è´¥ï¼Œè€—æ—¶ {} æ¯«ç§’",
         messages.size(),validLogs.

size(),failedMessages.

size(),duration);

// è½¬å‘æ—¥å¿—
        log.

info("è½¬å‘åˆ°æ£€æµ‹æ¨¡å—ï¼š{}/{}æ—¥å¿—æˆåŠŸ",successCount, logs.size());

// å†™å…¥æ—¥å¿—
        log.

info("æ‰¹é‡å†™å…¥å·²å®Œæˆï¼šæ€»è®¡={}, æˆåŠŸ={}, å¤±è´¥={}",
     logs.size(),totalSuccess,logs.

size() -totalSuccess);
```

---

## é…ç½®ç¤ºä¾‹

### å®Œæ•´é…ç½®

```yaml
server:
  port: 10250

spring:
  application:
    name: logx-engine-processor

  # Kafkaé…ç½®
  kafka:
    bootstrap-servers: localhost:29092
    consumer:
      group-id: logx-processor-group
      auto-offset-reset: latest
      enable-auto-commit: false
      max-poll-records: 500
      fetch-min-size: 1024
      fetch-max-wait: 500
      concurrency: 3

  # Elasticsearché…ç½®
  data:
    elasticsearch:
      uris: http://localhost:9200
      username: elastic
      password: 8rc3Jl1jlAK3uVZZyhF4
      connection-timeout: 10000
      socket-timeout: 30000

# LogXä¸šåŠ¡é…ç½®
logx:
  # Kafka Topic
  kafka:
    topic:
      log-ingestion: logx-logs
      log-processing: logx-logs-processing
      dead-letter: logx-logs-dlq

  # æ¶ˆè´¹è€…é…ç½®
  consumer:
    max-retries: 3
    retry-backoff-ms: 1000

  # ESæ‰¹é‡é…ç½®
  es:
    bulk:
      max-size: 500

# æ—¥å¿—é…ç½®
logging:
  level:
    com.domidodo.logx: DEBUG
  file:
    name: logs/processor.log
```

---

## æ•…éšœæ’æŸ¥

### 1. Kafkaæ¶ˆè´¹å †ç§¯

**ç°è±¡**: Kafka lag æŒç»­å¢é•¿

**æ’æŸ¥æ­¥éª¤**:

```bash
# 1. æ£€æŸ¥æ¶ˆè´¹è€…çŠ¶æ€
docker exec logx-kafka kafka-consumer-groups.sh \
  --bootstrap-server localhost:9092 \
  --describe --group logx-processor-group

# 2. æŸ¥çœ‹æ—¥å¿—
tail -f logs/processor.log | grep "å·²å¤„ç†"

# 3. æ£€æŸ¥ESæ€§èƒ½
curl http://localhost:9200/_cluster/health?pretty
```

**è§£å†³æ–¹æ¡ˆ**:

```yaml
# å¢åŠ å¹¶å‘æ¶ˆè´¹è€…
spring:
  kafka:
    consumer:
      concurrency: 5  # ä»3å¢åŠ åˆ°5
      max-poll-records: 1000  # å¢åŠ æ‰¹é‡
```

---

### 2. ESå†™å…¥å¤±è´¥

**ç°è±¡**: æ—¥å¿—ä¸­å¤§é‡ "å†™å…¥ESå¤±è´¥"

**æ’æŸ¥æ­¥éª¤**:

```bash
# 1. æ£€æŸ¥ESå¥åº·
curl http://localhost:9200/_cluster/health

# 2. æ£€æŸ¥ç´¢å¼•
curl http://localhost:9200/_cat/indices?v | grep logx-logs

# 3. æŸ¥çœ‹ESæ—¥å¿—
docker logs logx-es
```

**è§£å†³æ–¹æ¡ˆ**:

```yaml
# 1. å¢åŠ ESå †å†…å­˜
ES_JAVA_OPTS: "-Xms1g -Xmx1g"

# 2. å‡å°æ‰¹é‡å¤§å°
logx:
  es:
    bulk:
      max-size: 200  # ä»500å‡å°åˆ°200
```

---

### 3. å†…å­˜æº¢å‡º

**ç°è±¡**: `OutOfMemoryError`

**æ’æŸ¥æ­¥éª¤**:

```bash
# 1. æŸ¥çœ‹å †å†…å­˜
jmap -heap <pid>

# 2. ç”Ÿæˆå †è½¬å‚¨
jmap -dump:format=b,file=heap.bin <pid>

# 3. åˆ†æå†…å­˜
jhat heap.bin
```

**è§£å†³æ–¹æ¡ˆ**:

```bash
# å¢åŠ JVMå†…å­˜
java -Xms2g -Xmx4g -jar processor.jar

# æˆ–è®¾ç½®ç¯å¢ƒå˜é‡
export JAVA_OPTS="-Xms2g -Xmx4g"
```

---

## æœ€ä½³å®è·µ

### 1. æ€§èƒ½è°ƒä¼˜

```yaml
# é«˜ååé‡é…ç½®
spring:
  kafka:
    consumer:
      concurrency: 5
      max-poll-records: 1000

logx:
  es:
    bulk:
      max-size: 1000
      concurrent-requests: 4
```

### 2. å¯é æ€§é…ç½®

```yaml
# é«˜å¯é æ€§é…ç½®
spring:
  kafka:
    consumer:
      enable-auto-commit: false  # æ‰‹åŠ¨æäº¤

logx:
  consumer:
    max-retries: 5              # å¢åŠ é‡è¯•
    retry-backoff-ms: 2000      # å¢åŠ é—´éš”
```

### 3. èµ„æºè§„åˆ’

| ååé‡          | å¹¶å‘æ•° | æ‰¹é‡å¤§å° | å†…å­˜   | CPU |
|--------------|-----|------|------|-----|
| <1000/s      | 2   | 500  | 1GB  | 1æ ¸  |
| 1000-5000/s  | 3   | 500  | 2GB  | 2æ ¸  |
| 5000-10000/s | 5   | 1000 | 4GB  | 4æ ¸  |
| >10000/s     | 10+ | 1000 | 8GB+ | 8æ ¸+ |

---

## ä¸‹ä¸€æ­¥

- æŸ¥çœ‹ [Storageæ¨¡å—æ–‡æ¡£](./LogX-Storage-Guide.md) äº†è§£ç´¢å¼•ç”Ÿå‘½å‘¨æœŸç®¡ç†
- æŸ¥çœ‹ [Detectionæ¨¡å—æ–‡æ¡£](./LogX-Detection-Guide.md) äº†è§£å¼‚å¸¸æ£€æµ‹è§„åˆ™
- æŸ¥çœ‹ [ç›‘æ§æ–‡æ¡£](./LogX-Configuration-Guide.md) é…ç½®Prometheuså’ŒGrafana
